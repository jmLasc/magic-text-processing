{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will evaluate the text using TFIDF and some other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from basic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = fetchData(\"cards.json\")\n",
    "data = cleanData(raw)\n",
    "colors = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeBucket(listColor): # This takes ONE list, e.g. 'W'\n",
    "    tokensList = []\n",
    "    means = [] # This function also gets the mean!!\n",
    "\n",
    "    for cardDict in listColor:\n",
    "        if 'oracle_text' in list(cardDict.keys()):\n",
    "            tokens = nltk.word_tokenize(cardDict['oracle_text'])\n",
    "            for t in tokens:\n",
    "                tokensList.append(t)\n",
    "            means.append(len(tokens))\n",
    "\n",
    "    print(\"Mean = \" + str(np.mean(means)))\n",
    "\n",
    "    return tokensList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # will be a list(list(str))\n",
    "\n",
    "for i, key in enumerate(data.keys()):\n",
    "    print(colors[i])\n",
    "    tokens.append(tokenizeBucket(data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_common_ngrams(tokenlist, ngram, qty):\n",
    "\n",
    "    tokenlist = [w for w in tokenlist if w not in stoplist] \n",
    "\n",
    "    raw = nltk.ngrams(tokenlist, ngram)\n",
    "    fdist = nltk.FreqDist(raw)\n",
    "    for pair in fdist.most_common(qty): # print n most common bigrams\n",
    "        print(pair[0]) # most_common gives list(tuple(tuple(str, str), int)) so this is how we extract the words2\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = [\"{\", \"}\", 'W', 'U', 'B', 'R', 'G', \"T\", \"â€”\", \"TK\", \",\", \":\", \".\", \"'s\", \"a\", \"may\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tl in enumerate(tokens):\n",
    "    print(colors[i])\n",
    "    print_common_ngrams(tl, 6, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_collocations(tokenlist, n, f):\n",
    "    find = nltk.collocations.BigramCollocationFinder.from_words(tokenlist)\n",
    "    find.apply_freq_filter(f) \n",
    "    best = find.nbest(nltk.collocations.BigramAssocMeasures().pmi, 100)\n",
    "\n",
    "    for pair in best:\n",
    "        if pair[0][0].upper() == pair[0][0] and pair[1][0].upper() == pair[1][0]:\n",
    "            best.remove(pair)\n",
    "\n",
    "    print(\"** Common Collocations **\")\n",
    "    for i, pair in enumerate(best):\n",
    "        if i == n:\n",
    "            break\n",
    "        print(pair)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tl in enumerate(tokens):\n",
    "    print(colors[i])\n",
    "    print_collocations(tl, 15, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdir = os.path.join(os.getcwd(), 'cards')\n",
    "dirs = glob.glob(bigdir + \"/*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for dir in dirs:\n",
    "    # get some vars\n",
    "    list_txts = glob.glob(dir + \"/*.txt\")\n",
    "    list_names = [Path(text).stem for text in list_txts]\n",
    "\n",
    "    # make tdidf model + vectorize\n",
    "    tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(list_txts)\n",
    "\n",
    "    # make into df\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=list_names, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    tfidf_df.stack().reset_index()\n",
    "    tfidf_df = tfidf_df.stack().reset_index()\n",
    "    tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "    top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(15)\n",
    "    models.append(top_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = models[0]\n",
    "model_colorless = models[1]\n",
    "model_g = models[2]\n",
    "model_r = models[3]\n",
    "model_u = models[4]\n",
    "model_w = models[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
